{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T17:31:29.839303Z",
     "start_time": "2025-03-06T17:31:04.776386Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.read_csv(\"combined_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T17:31:31.883244Z",
     "start_time": "2025-03-06T17:31:29.840455Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Initialize a directed graph\n",
    "dialogue_graph = nx.DiGraph()\n",
    "\n",
    "# Add nodes and edges\n",
    "for idx, row in df_train.iterrows():\n",
    "    message_id = row[\"message_id\"]\n",
    "    parent_id = row[\"parent_id\"]\n",
    "    text = row[\"text\"]\n",
    "    role = row[\"role\"]\n",
    "\n",
    "    # Add node with attributes\n",
    "    dialogue_graph.add_node(message_id, text=text, role=role)\n",
    "\n",
    "    # Add edge if parent_id exists\n",
    "    if parent_id:\n",
    "        dialogue_graph.add_edge(parent_id, message_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T17:31:31.916237Z",
     "start_time": "2025-03-06T17:31:31.884014Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined_features\n",
      "<class 'str'>    39283\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_train[\"combined_features\"].apply(type).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T17:33:12.151428Z",
     "start_time": "2025-03-06T17:31:31.925498Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined_features\n",
      "<class 'torch.Tensor'>    39283\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import ast\n",
    "\n",
    "def string_to_tensor(tensor_string):\n",
    "    # Remove the \"tensor(\" prefix and \")\" suffix\n",
    "    tensor_string = tensor_string.replace(\"tensor(\", \"\").replace(\")\", \"\")\n",
    "\n",
    "    # Convert the string to a list of floats\n",
    "    tensor_list = ast.literal_eval(tensor_string)\n",
    "\n",
    "    # Convert the list to a PyTorch tensor\n",
    "    return torch.tensor(tensor_list, dtype=torch.float)\n",
    "\n",
    "# Apply the conversion to the \"combined_features\" column\n",
    "df_train[\"combined_features\"] = df_train[\"combined_features\"].apply(string_to_tensor)\n",
    "\n",
    "# Verify the conversion\n",
    "print(df_train[\"combined_features\"].apply(type).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T17:33:40.976379Z",
     "start_time": "2025-03-06T17:33:39.022178Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "combined_features_list=[]\n",
    "for _, row in df_train.iterrows():\n",
    "    combined_features_list.append(row[\"combined_features\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T17:33:41.622731Z",
     "start_time": "2025-03-06T17:33:41.013364Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "\n",
    "# Create a mapping from string node IDs to integer indices\n",
    "node_id_to_index = {node_id: idx for idx, node_id in enumerate(dialogue_graph.nodes)}\n",
    "# Convert edges to integer indices\n",
    "edges = [(node_id_to_index[src], node_id_to_index[dst]) for src, dst in dialogue_graph.edges]\n",
    "\n",
    "# Example output: [(0, 1), (2, 3), ..\n",
    "# Create edge index\n",
    "edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "# Create node features tensor\n",
    "node_features = torch.stack(combined_features_list)\n",
    "\n",
    "# Create PyTorch Geometric data object\n",
    "graph_data = Data(x=node_features, edge_index=edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T17:33:41.623369Z",
     "start_time": "2025-03-06T17:33:41.597443Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 39283\n"
     ]
    }
   ],
   "source": [
    "num_nodes = graph_data.x.size(0)\n",
    "print(f\"Number of nodes: {num_nodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T17:33:41.623606Z",
     "start_time": "2025-03-06T17:33:41.597528Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum node index in edge_index: 39378\n"
     ]
    }
   ],
   "source": [
    "max_index = torch.max(graph_data.edge_index).item()\n",
    "print(f\"Maximum node index in edge_index: {max_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T17:33:43.093668Z",
     "start_time": "2025-03-06T17:33:42.764477Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges after filtering: 39187\n"
     ]
    }
   ],
   "source": [
    "# Filter out invalid edges\n",
    "valid_mask = (graph_data.edge_index[0] < num_nodes) & (graph_data.edge_index[1] < num_nodes)\n",
    "graph_data.edge_index = graph_data.edge_index[:, valid_mask]\n",
    "\n",
    "# Update the number of edges\n",
    "num_edges = graph_data.edge_index.size(1)\n",
    "print(f\"Number of edges after filtering: {num_edges}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T17:33:45.033250Z",
     "start_time": "2025-03-06T17:33:44.769448Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 39283\n",
      "Maximum node index in edge_index: 39282\n"
     ]
    }
   ],
   "source": [
    "# Check the number of nodes and maximum node index\n",
    "num_nodes = graph_data.x.size(0)\n",
    "max_index = torch.max(graph_data.edge_index).item()\n",
    "print(f\"Number of nodes: {num_nodes}\")\n",
    "print(f\"Maximum node index in edge_index: {max_index}\")\n",
    "\n",
    "# Ensure max_index < num_nodes\n",
    "assert max_index < num_nodes, \"Invalid node indices in edge_index!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T17:33:47.306176Z",
     "start_time": "2025-03-06T17:33:47.017889Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes after reindexing: 39283\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.utils import coalesce\n",
    "\n",
    "# Reindex the nodes in edge_index\n",
    "unique_nodes, edge_index = torch.unique(graph_data.edge_index, return_inverse=True)\n",
    "edge_index = edge_index.reshape(2, -1)\n",
    "\n",
    "# Update the number of nodes\n",
    "num_nodes = unique_nodes.size(0)\n",
    "print(f\"Number of nodes after reindexing: {num_nodes}\")\n",
    "\n",
    "# Update graph_data\n",
    "graph_data.edge_index = edge_index\n",
    "graph_data.x = graph_data.x[unique_nodes]  # Reindex node features if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T10:47:24.863499Z",
     "start_time": "2025-03-05T10:47:24.824222Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)  # First GCN layer\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)  # Second GCN layer\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)  # First convolution\n",
    "        x = F.relu(x)  # Apply ReLU activation\n",
    "        x = self.conv2(x, edge_index)  # Second convolution\n",
    "        return x  # Output node embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T15:29:07.739688Z",
     "start_time": "2025-03-06T15:29:07.578060Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Input dimension: Size of the combined feature vector for each node\n",
    "input_dim = graph_data.x.size(1)\n",
    "\n",
    "# Hidden dimension: Size of the hidden layer (can be tuned)\n",
    "hidden_dim = 128\n",
    "\n",
    "# Output dimension: Size of the final node embeddings (can be tuned)\n",
    "output_dim = 64\n",
    "\n",
    "# Initialize the GCN model\n",
    "model = GCN(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T15:29:09.142786Z",
     "start_time": "2025-03-06T15:29:09.050776Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_loss(out):\n",
    "    # Reconstruct the adjacency matrix\n",
    "    adj_reconstructed = torch.sigmoid(torch.mm(out, out.t()))\n",
    "\n",
    "    # Compute reconstruction loss\n",
    "    loss = F.binary_cross_entropy(adj_reconstructed, torch.eye(graph_data.x.size(0), device=device))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.9450471997261047\n",
      "Epoch 2, Loss: 0.7464883923530579\n",
      "Epoch 3, Loss: 0.6054078340530396\n",
      "Epoch 4, Loss: 0.5619692206382751\n",
      "Early stopping due to no significant improvement in loss\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn.norm import BatchNorm\n",
    "\n",
    "# Define the GCN model\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.bn1 = BatchNorm(hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.bn2 = BatchNorm(hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, output_dim)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Input dimension: Size of the combined feature vector for each node\n",
    "input_dim = graph_data.x.size(1)\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_dim = 256  # Hidden dimension\n",
    "output_dim = 64  # Output dimension\n",
    "dropout = 0.5  # Dropout rate\n",
    "learning_rate = 0.001  # Learning rate\n",
    "weight_decay = 5e-4  # L2 regularization\n",
    "num_epochs = 100  # Number of epochs\n",
    "patience = 3  # Early stopping patience\n",
    "min_delta = 1  # Minimum change in the monitored quantity to qualify as an improvement\n",
    "\n",
    "# Initialize the GCN model\n",
    "model = GCN(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, dropout=dropout)\n",
    "\n",
    "# Move model and data to GPU (if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "graph_data = graph_data.to(device)\n",
    "\n",
    "# Normalize input features (to improve training stability)\n",
    "graph_data.x = (graph_data.x - graph_data.x.mean(dim=0)) / (graph_data.x.std(dim=0) + 1e-8)\n",
    "\n",
    "# Initialize the optimizer with weight decay (L2 regularization)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Loss function for graph reconstruction using MSE\n",
    "def compute_loss(out):\n",
    "    # Reconstruct the adjacency matrix\n",
    "    adj_reconstructed = torch.sigmoid(torch.mm(out, out.t()))\n",
    "\n",
    "    # Use the actual adjacency matrix as the target (if available)\n",
    "    adj_matrix = torch.sparse_coo_tensor(\n",
    "        graph_data.edge_index,\n",
    "        torch.ones(graph_data.edge_index.size(1)),\n",
    "        size=(graph_data.num_nodes, graph_data.num_nodes),\n",
    "    ).to_dense().to(device)\n",
    "\n",
    "    # Compute reconstruction loss using MSE\n",
    "    loss = F.mse_loss(adj_reconstructed, adj_matrix)\n",
    "    return loss\n",
    "\n",
    "# Early stopping criteria\n",
    "best_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()  # Reset gradients\n",
    "\n",
    "    out = model(graph_data.x, graph_data.edge_index)  # Forward pass\n",
    "    loss = compute_loss(out)  # Compute loss\n",
    "\n",
    "    loss.backward()  # Backward pass\n",
    "    optimizer.step()  # Update model parameters\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")  # Print loss\n",
    "\n",
    "    # Early stopping\n",
    "    if best_loss - loss.item() > min_delta:\n",
    "        best_loss = loss.item()\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping due to no significant improvement in loss\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.40867576003074646\n",
      "Epoch 2, Loss: 0.354690283536911\n",
      "Epoch 3, Loss: 0.3287122845649719\n",
      "Epoch 4, Loss: 0.3120560944080353\n",
      "Epoch 5, Loss: 0.3123176395893097\n",
      "Epoch 6, Loss: 0.30674678087234497\n",
      "Epoch 7, Loss: 0.29892855882644653\n",
      "Epoch 8, Loss: 0.2994576096534729\n",
      "Epoch 9, Loss: 0.31298717856407166\n",
      "Epoch 10, Loss: 0.28839296102523804\n",
      "Epoch 11, Loss: 0.2882095277309418\n",
      "Epoch 12, Loss: 0.28659510612487793\n",
      "Epoch 13, Loss: 0.28482022881507874\n",
      "Early stopping due to no significant improvement in loss\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GATConv\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn.norm import BatchNorm\n",
    "\n",
    "# Define the GAT model\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout, heads=1):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(input_dim, hidden_dim, heads=heads, dropout=dropout)\n",
    "        self.bn1 = BatchNorm(hidden_dim * heads)\n",
    "        self.conv2 = GATConv(hidden_dim * heads, hidden_dim, heads=heads, dropout=dropout)\n",
    "        self.bn2 = BatchNorm(hidden_dim * heads)\n",
    "        self.conv3 = GATConv(hidden_dim * heads, output_dim, heads=1, concat=False, dropout=dropout)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        \n",
    "        x = self.conv3(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Input dimension: Size of the combined feature vector for each node\n",
    "input_dim = graph_data.x.size(1)\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_dim = 256  # Hidden dimension\n",
    "output_dim = 64  # Output dimension\n",
    "dropout = 0.5  # Dropout rate\n",
    "learning_rate = 0.001  # Learning rate\n",
    "weight_decay = 5e-4  # L2 regularization\n",
    "num_epochs = 100  # Number of epochs\n",
    "patience = 3  # Early stopping patience\n",
    "min_delta = 0.01  # Minimum change in the monitored quantity to qualify as an improvement\n",
    "\n",
    "# Initialize the GAT model\n",
    "model = GAT(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, dropout=dropout)\n",
    "\n",
    "# Move model and data to GPU (if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "graph_data = graph_data.to(device)\n",
    "\n",
    "# Normalize input features (to improve training stability)\n",
    "graph_data.x = (graph_data.x - graph_data.x.mean(dim=0)) / (graph_data.x.std(dim=0) + 1e-8)\n",
    "\n",
    "# Initialize the optimizer with weight decay (L2 regularization)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Loss function for graph reconstruction using MSE\n",
    "def compute_loss(out):\n",
    "    # Reconstruct the adjacency matrix\n",
    "    adj_reconstructed = torch.sigmoid(torch.mm(out, out.t()))\n",
    "\n",
    "    # Use the actual adjacency matrix as the target (if available)\n",
    "    adj_matrix = torch.sparse_coo_tensor(\n",
    "        graph_data.edge_index,\n",
    "        torch.ones(graph_data.edge_index.size(1)),\n",
    "        size=(graph_data.num_nodes, graph_data.num_nodes),\n",
    "    ).to_dense().to(device)\n",
    "\n",
    "    # Compute reconstruction loss using MSE\n",
    "    loss = F.mse_loss(adj_reconstructed, adj_matrix)\n",
    "    return loss\n",
    "\n",
    "# Early stopping criteria\n",
    "best_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()  # Reset gradients\n",
    "\n",
    "    out = model(graph_data.x, graph_data.edge_index)  # Forward pass\n",
    "    loss = compute_loss(out)  # Compute loss\n",
    "\n",
    "    loss.backward()  # Backward pass\n",
    "    optimizer.step()  # Update model parameters\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")  # Print loss\n",
    "\n",
    "    # Early stopping\n",
    "    if best_loss - loss.item() > min_delta:\n",
    "        best_loss = loss.item()\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping due to no significant improvement in loss\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "node_embeddings = out.detach().cpu()\n",
    "torch.save(node_embeddings, \"node_embeddings.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_node_embeddings = torch.load(\"node_embeddings.pt\")\n",
    "\n",
    "# Assuming df_train is a DataFrame containing the message IDs\n",
    "df_train = pd.DataFrame({\"message_id\": range(len(loaded_node_embeddings))})  # Example DataFrame\n",
    "\n",
    "# Create the message_id_to_embedding dictionary\n",
    "message_id_to_embedding = {row[\"message_id\"]: loaded_node_embeddings[i] for i, row in df_train.iterrows()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.50.0-py3-none-any.whl.metadata (39 kB)\n",
      "Requirement already satisfied: filelock in /Users/mayur/myenv/lib/python3.13/site-packages (from transformers) (3.18.0)\n",
      "Collecting huggingface-hub<1.0,>=0.26.0 (from transformers)\n",
      "  Using cached huggingface_hub-0.29.3-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/mayur/myenv/lib/python3.13/site-packages (from transformers) (2.2.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/mayur/myenv/lib/python3.13/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/mayur/myenv/lib/python3.13/site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Using cached regex-2024.11.6-cp313-cp313-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /Users/mayur/myenv/lib/python3.13/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Using cached tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Using cached safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/mayur/myenv/lib/python3.13/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/mayur/myenv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/mayur/myenv/lib/python3.13/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/mayur/myenv/lib/python3.13/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/mayur/myenv/lib/python3.13/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/mayur/myenv/lib/python3.13/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/mayur/myenv/lib/python3.13/site-packages (from requests->transformers) (2025.1.31)\n",
      "Downloading transformers-4.50.0-py3-none-any.whl (10.2 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m1m22.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached huggingface_hub-0.29.3-py3-none-any.whl (468 kB)\n",
      "Using cached regex-2024.11.6-cp313-cp313-macosx_11_0_arm64.whl (284 kB)\n",
      "Using cached safetensors-0.5.3-cp38-abi3-macosx_11_0_arm64.whl (418 kB)\n",
      "Using cached tokenizers-0.21.1-cp39-abi3-macosx_11_0_arm64.whl (2.7 MB)\n",
      "Installing collected packages: safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.29.3 regex-2024.11.6 safetensors-0.5.3 tokenizers-0.21.1 transformers-4.50.0\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/Users/mayur/.cache/huggingface'\n",
      "Could not cache non-existence of file. Will ignore error and continue. Error: [Errno 13] Permission denied: '/Users/mayur/.cache/huggingface'\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "gpt2 does not appear to have a file named pytorch_model.bin but there is a file for TensorFlow weights. Use `from_tf=True` to load this model from those weights.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM, Trainer, TrainingArguments\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Load a pre-trained LLM\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m llm = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgpt2\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Define training arguments\u001b[39;00m\n\u001b[32m      7\u001b[39m training_args = TrainingArguments(\n\u001b[32m      8\u001b[39m     output_dir=\u001b[33m\"\u001b[39m\u001b[33m./results\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m      9\u001b[39m     num_train_epochs=\u001b[32m3\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     12\u001b[39m     save_total_limit=\u001b[32m2\u001b[39m,\n\u001b[32m     13\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.13/site-packages/transformers/models/auto/auto_factory.py:573\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(config) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m._model_mapping.keys():\n\u001b[32m    572\u001b[39m     model_class = _get_model_class(config, \u001b[38;5;28mcls\u001b[39m._model_mapping)\n\u001b[32m--> \u001b[39m\u001b[32m573\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    574\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    575\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    576\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    577\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    578\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    579\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.13/site-packages/transformers/modeling_utils.py:272\u001b[39m, in \u001b[36mrestore_default_torch_dtype.<locals>._wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    270\u001b[39m old_dtype = torch.get_default_dtype()\n\u001b[32m    271\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m272\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    274\u001b[39m     torch.set_default_dtype(old_dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.13/site-packages/transformers/modeling_utils.py:4317\u001b[39m, in \u001b[36mPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[39m\n\u001b[32m   4312\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   4313\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   4314\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mYou cannot combine Quantization and loading a model from a GGUF file, try again by making sure you did not passed a `quantization_config` or that you did not load a quantized model from the Hub.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   4315\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m4317\u001b[39m checkpoint_files, sharded_metadata = \u001b[43m_get_resolved_checkpoint_files\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4318\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4319\u001b[39m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m=\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4320\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4321\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgguf_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4322\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_tf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4323\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfrom_flax\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4324\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_safetensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4325\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4326\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4327\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4328\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4329\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4330\u001b[39m \u001b[43m    \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m=\u001b[49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4331\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4332\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4333\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4335\u001b[39m is_sharded = sharded_metadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4336\u001b[39m is_quantized = hf_quantizer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/myenv/lib/python3.13/site-packages/transformers/modeling_utils.py:1110\u001b[39m, in \u001b[36m_get_resolved_checkpoint_files\u001b[39m\u001b[34m(pretrained_model_name_or_path, subfolder, variant, gguf_file, from_tf, from_flax, use_safetensors, cache_dir, force_download, proxies, local_files_only, token, user_agent, revision, commit_hash)\u001b[39m\n\u001b[32m   1102\u001b[39m has_file_kwargs = {\n\u001b[32m   1103\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrevision\u001b[39m\u001b[33m\"\u001b[39m: revision,\n\u001b[32m   1104\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mproxies\u001b[39m\u001b[33m\"\u001b[39m: proxies,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1107\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mlocal_files_only\u001b[39m\u001b[33m\"\u001b[39m: local_files_only,\n\u001b[32m   1108\u001b[39m }\n\u001b[32m   1109\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_file(pretrained_model_name_or_path, TF2_WEIGHTS_NAME, **has_file_kwargs):\n\u001b[32m-> \u001b[39m\u001b[32m1110\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[32m   1111\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not appear to have a file named\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1112\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m but there is a file for TensorFlow weights.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1113\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m Use `from_tf=True` to load this model from those weights.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1114\u001b[39m     )\n\u001b[32m   1115\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m has_file(pretrained_model_name_or_path, FLAX_WEIGHTS_NAME, **has_file_kwargs):\n\u001b[32m   1116\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[32m   1117\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not appear to have a file named\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1118\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_add_variant(WEIGHTS_NAME,\u001b[38;5;250m \u001b[39mvariant)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m but there is a file for Flax weights. Use\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1119\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m `from_flax=True` to load this model from those weights.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1120\u001b[39m     )\n",
      "\u001b[31mOSError\u001b[39m: gpt2 does not appear to have a file named pytorch_model.bin but there is a file for TensorFlow weights. Use `from_tf=True` to load this model from those weights."
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "\n",
    "# Load a pre-trained LLM\n",
    "llm = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=llm,\n",
    "    args=training_args,\n",
    "    train_dataset=combined_inputs,  # Use combined inputs\n",
    ")\n",
    "\n",
    "# Fine-tune the LLM\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Generate responses\n",
    "generated_responses = llm.generate(input_ids=combined_inputs[\"input_ids\"], max_length=512)\n",
    "\n",
    "# Decode the generated responses\n",
    "decoded_responses = [tokenizer.decode(response, skip_special_tokens=True) for response in generated_responses]\n",
    "\n",
    "# Print the first few responses\n",
    "for i, response in enumerate(decoded_responses[:5]):\n",
    "    print(f\"Response {i + 1}: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# Example: Calculate BLEU score\n",
    "reference = df_train[\"text\"].tolist()  # Ground truth responses\n",
    "bleu_scores = [sentence_bleu([ref], gen) for ref, gen in zip(reference, decoded_responses)]\n",
    "average_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "print(f\"Average BLEU Score: {average_bleu}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
