{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "830e0785-acf2-474a-849f-c4e27fc50952",
      "metadata": {},
      "source": [
        "# "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "32882185-ee07-478a-a28c-130935d73e8c",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import ast\n",
        "import networkx as nx\n",
        "from torch_geometric.data import Data\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch_geometric.nn import GATConv, BatchNorm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "44046ed7-5eaf-4bbd-b7b7-8df97d22de7e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "df_train = pd.read_csv(\"combined_features.csv\")\n",
        "\n",
        "# Initialize a directed graph\n",
        "dialogue_graph = nx.DiGraph()\n",
        "\n",
        "for idx, row in df_train.iterrows():\n",
        "    message_id = row[\"message_id\"]\n",
        "    parent_id = row[\"parent_id\"]\n",
        "    text = row[\"text\"]\n",
        "    role = row[\"role\"]\n",
        "\n",
        "    # Add node with attributes\n",
        "    dialogue_graph.add_node(message_id, text=text, role=role)\n",
        "\n",
        "    if parent_id:\n",
        "        dialogue_graph.add_edge(parent_id, message_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "ab14d0be-18f8-4a83-a06f-5c0e95cf6a23",
      "metadata": {},
      "outputs": [],
      "source": [
        "def string_to_tensor(tensor_string):\n",
        "    tensor_string = tensor_string.replace(\"tensor(\", \"\").replace(\")\", \"\")\n",
        "    tensor_list = ast.literal_eval(tensor_string)\n",
        "    return torch.tensor(tensor_list, dtype=torch.float)\n",
        "\n",
        "df_train[\"combined_features\"] = df_train[\"combined_features\"].apply(string_to_tensor)\n",
        "\n",
        "node_id_to_index = {node_id: idx for idx, node_id in enumerate(dialogue_graph.nodes)}\n",
        "\n",
        "edges = [(node_id_to_index[src], node_id_to_index[dst]) for src, dst in dialogue_graph.edges]\n",
        "\n",
        "# Create edge index\n",
        "edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
        "\n",
        "node_features = torch.stack(df_train[\"combined_features\"].tolist())\n",
        "\n",
        "# Create PyTorch Geometric data object\n",
        "graph_data = Data(x=node_features, edge_index=edge_index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "d72e0713-7c25-4cc5-a689-112aa3949b03",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Filter out invalid edges\n",
        "num_nodes = graph_data.x.size(0)\n",
        "valid_mask = (graph_data.edge_index[0] < num_nodes) & (graph_data.edge_index[1] < num_nodes)\n",
        "graph_data.edge_index = graph_data.edge_index[:, valid_mask]\n",
        "\n",
        "original_message_ids = list(dialogue_graph.nodes)\n",
        "\n",
        "unique_nodes, edge_index = torch.unique(graph_data.edge_index, return_inverse=True)\n",
        "edge_index = edge_index.reshape(2, -1)\n",
        "\n",
        "# Update graph_data\n",
        "graph_data.edge_index = edge_index\n",
        "graph_data.x = graph_data.x[unique_nodes]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "e193654b-6c36-4a15-93db-5260da377165",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "class GAT(torch.nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim, output_dim, dropout, heads=1):\n",
        "        super(GAT, self).__init__()\n",
        "        self.conv1 = GATConv(input_dim, hidden_dim, heads=heads, dropout=dropout)\n",
        "        self.bn1 = BatchNorm(hidden_dim * heads)\n",
        "        self.conv2 = GATConv(hidden_dim * heads, hidden_dim, heads=heads, dropout=dropout)\n",
        "        self.bn2 = BatchNorm(hidden_dim * heads)\n",
        "        self.conv3 = GATConv(hidden_dim * heads, output_dim, heads=1, concat=False, dropout=dropout)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x, edge_index):\n",
        "        x = self.conv1(x, edge_index)\n",
        "        x = self.bn1(x)\n",
        "        x = F.elu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        \n",
        "        x = self.conv2(x, edge_index)\n",
        "        x = self.bn2(x)\n",
        "        x = F.elu(x)\n",
        "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
        "        \n",
        "        x = self.conv3(x, edge_index)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "7731d6b3-3edb-4d5d-a808-17176858c812",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['6ab24d72-0181-4594-a9cd-deaf170242fb', nan, 'c8e83833-ecbc-44fe-b6db-735228c25a1c', '6708c47f-05c9-4346-b3d2-40b2bd24fde4', '343ee2d4-87ae-41fd-a768-bdd65959dc4a', '18145bf4-37fd-4ac0-80f5-6108b5f2b365', '636dd191-50df-4894-ba9a-cd7f00767258', 'ac94bfcf-7f25-4084-8755-dde345ac2323', '73d6f715-3787-409c-81e4-fde0e5ef60cd', 'b280ccbc-b68f-42b9-9fc2-d7ac89b88022']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "\n",
        "input_dim = graph_data.x.size(1)\n",
        "\n",
        "# Hyperparameters\n",
        "hidden_dim = 256 \n",
        "output_dim = 64  \n",
        "dropout = 0.5  \n",
        "learning_rate = 0.001  \n",
        "weight_decay = 5e-4  \n",
        "num_epochs = 100  \n",
        "patience = 3  \n",
        "min_delta = 0.01  # Minimum change in the monitored quantity to qualify as an improvement\n",
        "\n",
        "model = GAT(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, dropout=dropout)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "graph_data = graph_data.to(device)\n",
        "\n",
        "# Normalize input features (to improve training stability)\n",
        "graph_data.x = (graph_data.x - graph_data.x.mean(dim=0)) / (graph_data.x.std(dim=0) + 1e-8)\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "# Loss function for graph reconstruction using MSE\n",
        "\n",
        "\n",
        "print(original_message_ids[:10])\n",
        "\n",
        "# Create a mapping of original message IDs to their corresponding embeddings\n",
        "# node_id_to_embedding = {msg_id: node_embeddings[unique_nodes.tolist().index(node_id_to_index[msg_id])] for msg_id in original_message_ids}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "3e743a28-75f6-49bd-8fb2-8dd93ff56384",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 0.4016643762588501\n",
            "Epoch 2, Loss: 0.35843244194984436\n",
            "Epoch 3, Loss: 0.3413124680519104\n",
            "Epoch 4, Loss: 0.33853679895401\n",
            "Epoch 5, Loss: 0.3158513903617859\n",
            "Epoch 6, Loss: 0.309251993894577\n",
            "Epoch 7, Loss: 0.3082663416862488\n",
            "Epoch 8, Loss: 0.2996441721916199\n",
            "Epoch 9, Loss: 0.2912551164627075\n",
            "Epoch 10, Loss: 0.2846744656562805\n",
            "Epoch 11, Loss: 0.28691428899765015\n",
            "Epoch 12, Loss: 0.28918108344078064\n",
            "Epoch 13, Loss: 0.288006454706192\n",
            "Early stopping due to no significant improvement in loss\n"
          ]
        }
      ],
      "source": [
        "def compute_loss(out):\n",
        "    adj_reconstructed = torch.sigmoid(torch.mm(out, out.t()))\n",
        "    adj_matrix = torch.sparse_coo_tensor(\n",
        "        graph_data.edge_index,\n",
        "        torch.ones(graph_data.edge_index.size(1)),\n",
        "        size=(graph_data.num_nodes, graph_data.num_nodes),\n",
        "    ).to_dense().to(device)\n",
        "    loss = F.mse_loss(adj_reconstructed, adj_matrix)\n",
        "    return loss\n",
        "\n",
        "# Early stopping criteria\n",
        "best_loss = float('inf')\n",
        "patience_counter = 0\n",
        "\n",
        "# Training loop\n",
        "model.train()\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    out = model(graph_data.x, graph_data.edge_index)\n",
        "    loss = compute_loss(out)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")\n",
        "    if best_loss - loss.item() > min_delta:\n",
        "        best_loss = loss.item()\n",
        "        patience_counter = 0\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "    if patience_counter >= patience:\n",
        "        print(\"Early stopping due to no significant improvement in loss\")\n",
        "        break\n",
        "\n",
        "# Extract node embeddings\n",
        "node_embeddings = out.detach().cpu()\n",
        "\n",
        "# Save node embeddings\n",
        "torch.save(node_embeddings, \"node_embeddings.pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "12ae8ee6-6290-40bb-90e0-5e8c0583cc22",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample of node_id_to_index: [('6ab24d72-0181-4594-a9cd-deaf170242fb', 0), (nan, 1), ('c8e83833-ecbc-44fe-b6db-735228c25a1c', 2), ('6708c47f-05c9-4346-b3d2-40b2bd24fde4', 3), ('343ee2d4-87ae-41fd-a768-bdd65959dc4a', 4), ('18145bf4-37fd-4ac0-80f5-6108b5f2b365', 5), ('636dd191-50df-4894-ba9a-cd7f00767258', 6), ('ac94bfcf-7f25-4084-8755-dde345ac2323', 7), ('73d6f715-3787-409c-81e4-fde0e5ef60cd', 8), ('b280ccbc-b68f-42b9-9fc2-d7ac89b88022', 9)]\n",
            "unique_nodes: tensor([    0,     1,     2,  ..., 39280, 39281, 39282])\n",
            "Missing indices in unique_nodes: [39283, 39284, 39285, 39286, 39287, 39288, 39289, 39290, 39291, 39292, 39293, 39294, 39295, 39296, 39297, 39298, 39299, 39300, 39301, 39302, 39303, 39304, 39305, 39306, 39307, 39308, 39309, 39310, 39311, 39312, 39313, 39314, 39315, 39316, 39317, 39318, 39319, 39320, 39321, 39322, 39323, 39324, 39325, 39326, 39327, 39328, 39329, 39330, 39331, 39332, 39333, 39334, 39335, 39336, 39337, 39338, 39339, 39340, 39341, 39342, 39343, 39344, 39345, 39346, 39347, 39348, 39349, 39350, 39351, 39352, 39353, 39354, 39355, 39356, 39357, 39358, 39359, 39360, 39361, 39362, 39363, 39364, 39365, 39366, 39367, 39368, 39369, 39370, 39371, 39372, 39373, 39374, 39375, 39376, 39377, 39378]\n"
          ]
        }
      ],
      "source": [
        "print(\"Sample of node_id_to_index:\", list(node_id_to_index.items())[:10])\n",
        "print(\"unique_nodes:\", unique_nodes)\n",
        "missing_indices = [idx for idx in node_id_to_index.values() if idx not in unique_nodes.tolist()]\n",
        "print(\"Missing indices in unique_nodes:\", missing_indices)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "fdd94389-798e-4dc1-815e-2faf0acba0b6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Problematic message ID: 0e06098e-7b5c-471b-8ee2-853cebc3ed58\n"
          ]
        }
      ],
      "source": [
        "problematic_msg_id = None\n",
        "for msg_id in original_message_ids:\n",
        "    try:\n",
        "        node_index = node_id_to_index[msg_id]\n",
        "        unique_nodes.tolist().index(node_index)\n",
        "    except ValueError:\n",
        "        problematic_msg_id = msg_id\n",
        "        break\n",
        "\n",
        "print(\"Problematic message ID:\", problematic_msg_id)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "ae059c51-7f00-402e-9804-852e56d8e8bd",
      "metadata": {},
      "outputs": [],
      "source": [
        "node_id_to_embedding = {}\n",
        "for msg_id in original_message_ids:\n",
        "    node_index = node_id_to_index[msg_id]\n",
        "    if node_index in unique_nodes.tolist():\n",
        "        embedding = node_embeddings[unique_nodes.tolist().index(node_index)]\n",
        "    else:\n",
        "        embedding = torch.zeros(output_dim)  # Default embedding if not found\n",
        "    node_id_to_embedding[msg_id] = embedding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "73fcc868-b831-4122-b97e-ac32db406816",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                             message_id                             parent_id  \\\n",
            "0  6ab24d72-0181-4594-a9cd-deaf170242fb                                   NaN   \n",
            "1  c8e83833-ecbc-44fe-b6db-735228c25a1c  6ab24d72-0181-4594-a9cd-deaf170242fb   \n",
            "2  6708c47f-05c9-4346-b3d2-40b2bd24fde4  c8e83833-ecbc-44fe-b6db-735228c25a1c   \n",
            "3  343ee2d4-87ae-41fd-a768-bdd65959dc4a  6ab24d72-0181-4594-a9cd-deaf170242fb   \n",
            "4  18145bf4-37fd-4ac0-80f5-6108b5f2b365  343ee2d4-87ae-41fd-a768-bdd65959dc4a   \n",
            "\n",
            "                                                text       role  \\\n",
            "0  Can you write a short introduction about the r...   prompter   \n",
            "1  \"Monopsony\" refers to a market structure where...  assistant   \n",
            "2                            Now explain it to a dog   prompter   \n",
            "3  Monopsony is a market structure in which there...  assistant   \n",
            "4  How can one fight back when a monospony had be...   prompter   \n",
            "\n",
            "                                           embedding  \n",
            "0  [tensor(0.1143), tensor(0.0116), tensor(0.0887...  \n",
            "1  [tensor(0.0352), tensor(0.0889), tensor(0.1812...  \n",
            "2  [tensor(-0.7281), tensor(-2.2154), tensor(1.75...  \n",
            "3  [tensor(0.1243), tensor(0.2893), tensor(-0.180...  \n",
            "4  [tensor(-1.7989), tensor(-3.6644), tensor(-0.8...  \n",
            "Updated DataFrame saved successfully!\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "# Define the dimensions of the embeddings\n",
        "output_dim = 64  \n",
        "\n",
        "df_train = pd.read_csv(\"OpenAssistant_English_Train.csv\")\n",
        "\n",
        "# node_id_to_embedding = torch.load(\"path_to_node_embeddings.pth\")  # Load your embeddings dictionary\n",
        "\n",
        "# Add embeddings to the DataFrame\n",
        "df_train[\"embedding\"] = df_train[\"message_id\"].apply(lambda x: node_id_to_embedding.get(x, torch.zeros(output_dim)))\n",
        "\n",
        "print(df_train.head())\n",
        "\n",
        "df_train.to_csv(\"OpenAssistant_English_Train_with_Embeddings.csv\", index=False)\n",
        "\n",
        "print(\"Updated DataFrame saved successfully!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "ce6d55fc-4422-4ded-a39c-988cbc25139f",
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from langchain_ollama import OllamaLLM\n",
        "import ast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "7a09608a-64e3-4a7d-905b-ea96913dddfe",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Initialize the OllamaLLM with the LLaMA 3 model\n",
        "llm = OllamaLLM(model=\"llama3\")\n",
        "\n",
        "def get_assistant_response(prompt, embedding=None):\n",
        "    if embedding is not None:\n",
        "        # Convert embedding to a string format\n",
        "        embedding_str = \" \".join([f\"{x:.4f}\" for x in embedding])\n",
        "        # Combine prompt and embedding with an instruction for a short response\n",
        "        combined_prompt = f\"{prompt}\\n\\n[Embedding: {embedding_str}]\\nPlease provide a short response.\"\n",
        "        response = llm.invoke(combined_prompt)\n",
        "    else:\n",
        "        # Add an instruction for a short response\n",
        "        combined_prompt = f\"{prompt}\\nPlease provide a short response.\"\n",
        "        response = llm.invoke(combined_prompt)\n",
        "    return response\n",
        "\n",
        "# Create a new DataFrame to store the results\n",
        "results = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a63962e9-2087-4448-be99-be505d3c41a0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipykernel_58682/2350718363.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  node_embedding = torch.tensor(row['embedding'])  # Convert the list to a tensor if necessary\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2\n",
            "4\n",
            "6\n",
            "10\n",
            "12\n",
            "14\n"
          ]
        }
      ],
      "source": [
        "# Iterate through the dataset and pair prompter messages with their respective assistant responses\n",
        "\n",
        "df = df_train\n",
        "\n",
        "for index, row in df.iterrows():\n",
        "    if row['role'] == 'prompter':\n",
        "        prompt = row['text']\n",
        "        # Find the corresponding assistant response\n",
        "        assistant_response = df.loc[(df['parent_id'] == row['message_id']) & (df['role'] == 'assistant'), 'text'].values\n",
        "        if len(assistant_response) > 0:\n",
        "            assistant_response = assistant_response[0]\n",
        "        else:\n",
        "            assistant_response = None\n",
        "        if(index>100):\n",
        "            break\n",
        "        print(index)\n",
        "        \n",
        "        # Generate responses with and without embeddings\n",
        "        node_embedding = torch.tensor(row['embedding'])  # Convert the list to a tensor if necessary\n",
        "        response_with_embedding = get_assistant_response(prompt, node_embedding)\n",
        "        response_without_embedding = get_assistant_response(prompt)\n",
        "        \n",
        "        # Append the results to the new DataFrame\n",
        "        results.append({\n",
        "            'message_id': row['message_id'],\n",
        "            'parent_id': row['parent_id'],\n",
        "            'prompt': prompt,\n",
        "            'assistant_response': assistant_response,\n",
        "            'generated_response_with_embedding': response_with_embedding,\n",
        "            'generated_response_without_embedding': response_without_embedding\n",
        "        })\n",
        "\n",
        "# Convert the results to a DataFrame\n",
        "results_df = pd.DataFrame(results)\n",
        "\n",
        "# Save the results to a new CSV file\n",
        "results_df.to_csv(\"results2.csv\", index=False)\n",
        "\n",
        "print(\"Results saved successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68648255-de2c-4e31-85cd-b2eb12b4bdd5",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (myenv)",
      "language": "python",
      "name": "myenv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
