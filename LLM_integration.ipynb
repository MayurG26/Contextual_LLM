{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2025-03-29T12:35:29.645830Z",
     "end_time": "2025-03-29T12:37:44.814451Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([39283, 64])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Load the GNN embeddings from .pt file\n",
    "gnn_embeddings = torch.load(\"node_embeddings.pt\")\n",
    "\n",
    "# Check the shape\n",
    "print(gnn_embeddings.shape)  # Example: (num_dialogues, embedding_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8fe5a6e2cf7e4b26930aa633a9f7ae17"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ad6ff50d2bbe43f499d60b81dbd47053"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "README.md:   0%|          | 0.00/10.5k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fbef0944439f4bca96d22ac7e7f1998d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0e96ad9882014e268a5dfab57210cf59"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "afe6cc4b099a46f5a05765ec3aedc210"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "14c1d1b01daf4fee8c65393e453de083"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3a48f80c09f34ffeae0c2a221fdee4a1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "40c71dcd6de6449abb4d999432d8decc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "40811c7e944f4d6ea1945ebf805ed5ec"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5349ff49e685493e8ef35f5779b38bd8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f3a7933945554dba92c615b22d054d22"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Convert GNN embeddings to NumPy\n",
    "embeddings_np = gnn_embeddings.numpy()\n",
    "\n",
    "# Create FAISS index\n",
    "index = faiss.IndexFlatL2(embeddings_np.shape[1])\n",
    "index.add(embeddings_np)\n",
    "\n",
    "# Load a text embedding model (to embed new queries)\n",
    "text_model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "def retrieve_similar_questions(query, k=5):\n",
    "    query_embedding = text_model.encode([query])  # Convert text to embedding\n",
    "    D, I = index.search(query_embedding, k)  # Retrieve top-k similar dialogues\n",
    "    return I[0]  # Return indices of most relevant dialogues\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2025-03-29T13:47:29.202275Z",
     "end_time": "2025-03-29T13:49:42.621911Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "009e14d8dd9f4312a3c0edadb5124902"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# Load a pre-trained LLM (e.g., Mistral, LLaMA)\n",
    "model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "# Example question\n",
    "user_question = \"How do neural networks generalize?\"\n",
    "\n",
    "# Retrieve structurally similar dialogues\n",
    "retrieved_indices = retrieve_similar_questions(user_question)\n",
    "\n",
    "# Modify input prompt\n",
    "structured_input = modify_llm_input(user_question, retrieved_indices, dialogues)\n",
    "\n",
    "# Tokenize and generate response\n",
    "input_ids = tokenizer(structured_input, return_tensors=\"pt\").input_ids\n",
    "output = model.generate(input_ids, max_length=200)\n",
    "response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"LLM Response:\", response)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
