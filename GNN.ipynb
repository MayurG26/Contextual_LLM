{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T17:31:29.839303Z",
     "start_time": "2025-03-06T17:31:04.776386Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_train = pd.read_csv(\"combined_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T17:31:31.883244Z",
     "start_time": "2025-03-06T17:31:29.840455Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "# Initialize a directed graph\n",
    "dialogue_graph = nx.DiGraph()\n",
    "\n",
    "# Add nodes and edges\n",
    "for idx, row in df_train.iterrows():\n",
    "    message_id = row[\"message_id\"]\n",
    "    parent_id = row[\"parent_id\"]\n",
    "    text = row[\"text\"]\n",
    "    role = row[\"role\"]\n",
    "\n",
    "    # Add node with attributes\n",
    "    dialogue_graph.add_node(message_id, text=text, role=role)\n",
    "\n",
    "    # Add edge if parent_id exists\n",
    "    if parent_id:\n",
    "        dialogue_graph.add_edge(parent_id, message_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T17:31:31.916237Z",
     "start_time": "2025-03-06T17:31:31.884014Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined_features\n",
      "<class 'str'>    39283\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_train[\"combined_features\"].apply(type).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T17:33:12.151428Z",
     "start_time": "2025-03-06T17:31:31.925498Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "combined_features\n",
      "<class 'torch.Tensor'>    39283\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import ast\n",
    "\n",
    "def string_to_tensor(tensor_string):\n",
    "    # Remove the \"tensor(\" prefix and \")\" suffix\n",
    "    tensor_string = tensor_string.replace(\"tensor(\", \"\").replace(\")\", \"\")\n",
    "\n",
    "    # Convert the string to a list of floats\n",
    "    tensor_list = ast.literal_eval(tensor_string)\n",
    "\n",
    "    # Convert the list to a PyTorch tensor\n",
    "    return torch.tensor(tensor_list, dtype=torch.float)\n",
    "\n",
    "# Apply the conversion to the \"combined_features\" column\n",
    "df_train[\"combined_features\"] = df_train[\"combined_features\"].apply(string_to_tensor)\n",
    "\n",
    "# Verify the conversion\n",
    "print(df_train[\"combined_features\"].apply(type).value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T17:33:40.976379Z",
     "start_time": "2025-03-06T17:33:39.022178Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "combined_features_list=[]\n",
    "for _, row in df_train.iterrows():\n",
    "    combined_features_list.append(row[\"combined_features\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T17:33:41.622731Z",
     "start_time": "2025-03-06T17:33:41.013364Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from torch_geometric.data import Data\n",
    "\n",
    "# Create a mapping from string node IDs to integer indices\n",
    "node_id_to_index = {node_id: idx for idx, node_id in enumerate(dialogue_graph.nodes)}\n",
    "# Convert edges to integer indices\n",
    "edges = [(node_id_to_index[src], node_id_to_index[dst]) for src, dst in dialogue_graph.edges]\n",
    "\n",
    "# Example output: [(0, 1), (2, 3), ..\n",
    "# Create edge index\n",
    "edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "# Create node features tensor\n",
    "node_features = torch.stack(combined_features_list)\n",
    "\n",
    "# Create PyTorch Geometric data object\n",
    "graph_data = Data(x=node_features, edge_index=edge_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T17:33:41.623369Z",
     "start_time": "2025-03-06T17:33:41.597443Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 39283\n"
     ]
    }
   ],
   "source": [
    "num_nodes = graph_data.x.size(0)\n",
    "print(f\"Number of nodes: {num_nodes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T17:33:41.623606Z",
     "start_time": "2025-03-06T17:33:41.597528Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum node index in edge_index: 39378\n"
     ]
    }
   ],
   "source": [
    "max_index = torch.max(graph_data.edge_index).item()\n",
    "print(f\"Maximum node index in edge_index: {max_index}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T17:33:43.093668Z",
     "start_time": "2025-03-06T17:33:42.764477Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of edges after filtering: 39187\n"
     ]
    }
   ],
   "source": [
    "# Filter out invalid edges\n",
    "valid_mask = (graph_data.edge_index[0] < num_nodes) & (graph_data.edge_index[1] < num_nodes)\n",
    "graph_data.edge_index = graph_data.edge_index[:, valid_mask]\n",
    "\n",
    "# Update the number of edges\n",
    "num_edges = graph_data.edge_index.size(1)\n",
    "print(f\"Number of edges after filtering: {num_edges}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T17:33:45.033250Z",
     "start_time": "2025-03-06T17:33:44.769448Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes: 39283\n",
      "Maximum node index in edge_index: 39282\n"
     ]
    }
   ],
   "source": [
    "# Check the number of nodes and maximum node index\n",
    "num_nodes = graph_data.x.size(0)\n",
    "max_index = torch.max(graph_data.edge_index).item()\n",
    "print(f\"Number of nodes: {num_nodes}\")\n",
    "print(f\"Maximum node index in edge_index: {max_index}\")\n",
    "\n",
    "# Ensure max_index < num_nodes\n",
    "assert max_index < num_nodes, \"Invalid node indices in edge_index!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T17:33:47.306176Z",
     "start_time": "2025-03-06T17:33:47.017889Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nodes after reindexing: 39283\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.utils import coalesce\n",
    "\n",
    "# Reindex the nodes in edge_index\n",
    "unique_nodes, edge_index = torch.unique(graph_data.edge_index, return_inverse=True)\n",
    "edge_index = edge_index.reshape(2, -1)\n",
    "\n",
    "# Update the number of nodes\n",
    "num_nodes = unique_nodes.size(0)\n",
    "print(f\"Number of nodes after reindexing: {num_nodes}\")\n",
    "\n",
    "# Update graph_data\n",
    "graph_data.edge_index = edge_index\n",
    "graph_data.x = graph_data.x[unique_nodes]  # Reindex node features if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-05T10:47:24.863499Z",
     "start_time": "2025-03-05T10:47:24.824222Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)  # First GCN layer\n",
    "        self.conv2 = GCNConv(hidden_dim, output_dim)  # Second GCN layer\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)  # First convolution\n",
    "        x = F.relu(x)  # Apply ReLU activation\n",
    "        x = self.conv2(x, edge_index)  # Second convolution\n",
    "        return x  # Output node embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T15:29:07.739688Z",
     "start_time": "2025-03-06T15:29:07.578060Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Input dimension: Size of the combined feature vector for each node\n",
    "input_dim = graph_data.x.size(1)\n",
    "\n",
    "# Hidden dimension: Size of the hidden layer (can be tuned)\n",
    "hidden_dim = 128\n",
    "\n",
    "# Output dimension: Size of the final node embeddings (can be tuned)\n",
    "output_dim = 64\n",
    "\n",
    "# Initialize the GCN model\n",
    "model = GCN(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-06T15:29:09.142786Z",
     "start_time": "2025-03-06T15:29:09.050776Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def compute_loss(out):\n",
    "    # Reconstruct the adjacency matrix\n",
    "    adj_reconstructed = torch.sigmoid(torch.mm(out, out.t()))\n",
    "\n",
    "    # Compute reconstruction loss\n",
    "    loss = F.binary_cross_entropy(adj_reconstructed, torch.eye(graph_data.x.size(0), device=device))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn.norm import BatchNorm\n",
    "\n",
    "# Define the GCN model\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.bn1 = BatchNorm(hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "        self.bn2 = BatchNorm(hidden_dim)\n",
    "        self.conv3 = GCNConv(hidden_dim, output_dim)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        x = self.conv3(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Input dimension: Size of the combined feature vector for each node\n",
    "input_dim = graph_data.x.size(1)\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_dim = 256  # Hidden dimension\n",
    "output_dim = 64  # Output dimension\n",
    "dropout = 0.5  # Dropout rate\n",
    "learning_rate = 0.001  # Learning rate\n",
    "weight_decay = 5e-4  # L2 regularization\n",
    "num_epochs = 100  # Number of epochs\n",
    "patience = 3  # Early stopping patience\n",
    "min_delta = 1  # Minimum change in the monitored quantity to qualify as an improvement\n",
    "\n",
    "# Initialize the GCN model\n",
    "model = GCN(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, dropout=dropout)\n",
    "\n",
    "# Move model and data to GPU (if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "graph_data = graph_data.to(device)\n",
    "\n",
    "# Normalize input features (to improve training stability)\n",
    "graph_data.x = (graph_data.x - graph_data.x.mean(dim=0)) / (graph_data.x.std(dim=0) + 1e-8)\n",
    "\n",
    "# Initialize the optimizer with weight decay (L2 regularization)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Loss function for graph reconstruction using MSE\n",
    "def compute_loss(out):\n",
    "    # Reconstruct the adjacency matrix\n",
    "    adj_reconstructed = torch.sigmoid(torch.mm(out, out.t()))\n",
    "\n",
    "    # Use the actual adjacency matrix as the target (if available)\n",
    "    adj_matrix = torch.sparse_coo_tensor(\n",
    "        graph_data.edge_index,\n",
    "        torch.ones(graph_data.edge_index.size(1)),\n",
    "        size=(graph_data.num_nodes, graph_data.num_nodes),\n",
    "    ).to_dense().to(device)\n",
    "\n",
    "    # Compute reconstruction loss using MSE\n",
    "    loss = F.mse_loss(adj_reconstructed, adj_matrix)\n",
    "    return loss\n",
    "\n",
    "# Early stopping criteria\n",
    "best_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()  # Reset gradients\n",
    "\n",
    "    out = model(graph_data.x, graph_data.edge_index)  # Forward pass\n",
    "    loss = compute_loss(out)  # Compute loss\n",
    "\n",
    "    loss.backward()  # Backward pass\n",
    "    optimizer.step()  # Update model parameters\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")  # Print loss\n",
    "\n",
    "    # Early stopping\n",
    "    if best_loss - loss.item() > min_delta:\n",
    "        best_loss = loss.item()\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping due to no significant improvement in loss\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.37238770723342896\n",
      "Epoch 2, Loss: 0.34143513441085815\n",
      "Epoch 3, Loss: 0.3216991722583771\n",
      "Epoch 4, Loss: 0.3262239396572113\n",
      "Epoch 5, Loss: 0.31101688742637634\n",
      "Epoch 6, Loss: 0.30912816524505615\n",
      "Epoch 7, Loss: 0.29736435413360596\n",
      "Epoch 8, Loss: 0.2934226095676422\n",
      "Epoch 9, Loss: 0.29253339767456055\n",
      "Epoch 10, Loss: 0.2868081331253052\n",
      "Epoch 11, Loss: 0.27935338020324707\n",
      "Epoch 12, Loss: 0.2771473526954651\n",
      "Epoch 13, Loss: 0.2720472514629364\n",
      "Epoch 14, Loss: 0.26962873339653015\n",
      "Epoch 15, Loss: 0.27165481448173523\n",
      "Epoch 16, Loss: 0.2674511671066284\n",
      "Early stopping due to no significant improvement in loss\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch_geometric.nn import GATConv\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn.norm import BatchNorm\n",
    "\n",
    "# Define the GAT model\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, dropout, heads=1):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(input_dim, hidden_dim, heads=heads, dropout=dropout)\n",
    "        self.bn1 = BatchNorm(hidden_dim * heads)\n",
    "        self.conv2 = GATConv(hidden_dim * heads, hidden_dim, heads=heads, dropout=dropout)\n",
    "        self.bn2 = BatchNorm(hidden_dim * heads)\n",
    "        self.conv3 = GATConv(hidden_dim * heads, output_dim, heads=1, concat=False, dropout=dropout)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = self.bn1(x)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        \n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = self.bn2(x)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        \n",
    "        x = self.conv3(x, edge_index)\n",
    "        return x\n",
    "\n",
    "# Input dimension: Size of the combined feature vector for each node\n",
    "input_dim = graph_data.x.size(1)\n",
    "\n",
    "# Hyperparameters\n",
    "hidden_dim = 256  # Hidden dimension\n",
    "output_dim = 64  # Output dimension\n",
    "dropout = 0.5  # Dropout rate\n",
    "learning_rate = 0.001  # Learning rate\n",
    "weight_decay = 5e-4  # L2 regularization\n",
    "num_epochs = 100  # Number of epochs\n",
    "patience = 3  # Early stopping patience\n",
    "min_delta = 0.01  # Minimum change in the monitored quantity to qualify as an improvement\n",
    "\n",
    "# Initialize the GAT model\n",
    "model = GAT(input_dim=input_dim, hidden_dim=hidden_dim, output_dim=output_dim, dropout=dropout)\n",
    "\n",
    "# Move model and data to GPU (if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "graph_data = graph_data.to(device)\n",
    "\n",
    "# Normalize input features (to improve training stability)\n",
    "graph_data.x = (graph_data.x - graph_data.x.mean(dim=0)) / (graph_data.x.std(dim=0) + 1e-8)\n",
    "\n",
    "# Initialize the optimizer with weight decay (L2 regularization)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "# Loss function for graph reconstruction using MSE\n",
    "def compute_loss(out):\n",
    "    # Reconstruct the adjacency matrix\n",
    "    adj_reconstructed = torch.sigmoid(torch.mm(out, out.t()))\n",
    "\n",
    "    # Use the actual adjacency matrix as the target (if available)\n",
    "    adj_matrix = torch.sparse_coo_tensor(\n",
    "        graph_data.edge_index,\n",
    "        torch.ones(graph_data.edge_index.size(1)),\n",
    "        size=(graph_data.num_nodes, graph_data.num_nodes),\n",
    "    ).to_dense().to(device)\n",
    "\n",
    "    # Compute reconstruction loss using MSE\n",
    "    loss = F.mse_loss(adj_reconstructed, adj_matrix)\n",
    "    return loss\n",
    "\n",
    "# Early stopping criteria\n",
    "best_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()  # Reset gradients\n",
    "\n",
    "    out = model(graph_data.x, graph_data.edge_index)  # Forward pass\n",
    "    loss = compute_loss(out)  # Compute loss\n",
    "\n",
    "    loss.backward()  # Backward pass\n",
    "    optimizer.step()  # Update model parameters\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Loss: {loss.item()}\")  # Print loss\n",
    "\n",
    "    # Early stopping\n",
    "    if best_loss - loss.item() > min_delta:\n",
    "        best_loss = loss.item()\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter >= patience:\n",
    "        print(\"Early stopping due to no significant improvement in loss\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GAT(\n",
      "  (conv1): GATConv(774, 256, heads=1)\n",
      "  (bn1): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv2): GATConv(256, 256, heads=1)\n",
      "  (bn2): BatchNorm(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (conv3): GATConv(256, 64, heads=1)\n",
      ")\n",
      "Node embedding shape :  torch.Size([39283, 64])\n"
     ]
    }
   ],
   "source": [
    "print(model.eval())\n",
    "\n",
    "with torch.no_grad():\n",
    "    node_embeddings = model(graph_data.x,graph_data.edge_index)\n",
    "\n",
    "print(\"Node embedding shape : \", node_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to map message IDs to node embeddings\n",
    "message_id_to_embedding = {row[\"message_id\"]: node_embeddings[i] for i, row in df_train.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "\n",
    "# Load a pre-trained LLM\n",
    "llm = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=llm,\n",
    "    args=training_args,\n",
    "    train_dataset=combined_inputs,  # Use combined inputs\n",
    ")\n",
    "\n",
    "# Fine-tune the LLM\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Generate responses\n",
    "generated_responses = llm.generate(input_ids=combined_inputs[\"input_ids\"], max_length=512)\n",
    "\n",
    "# Decode the generated responses\n",
    "decoded_responses = [tokenizer.decode(response, skip_special_tokens=True) for response in generated_responses]\n",
    "\n",
    "# Print the first few responses\n",
    "for i, response in enumerate(decoded_responses[:5]):\n",
    "    print(f\"Response {i + 1}: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# Example: Calculate BLEU score\n",
    "reference = df_train[\"text\"].tolist()  # Ground truth responses\n",
    "bleu_scores = [sentence_bleu([ref], gen) for ref, gen in zip(reference, decoded_responses)]\n",
    "average_bleu = sum(bleu_scores) / len(bleu_scores)\n",
    "print(f\"Average BLEU Score: {average_bleu}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
